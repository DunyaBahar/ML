import java.util.Random;

// topology agnostic
// dont have training set, don't know # features or input nodes
// dont know what target label or output node is
// can't set up topology in constructor 
public class NeuralNet extends SupervisedLearner {
	Random random;
	// topology here
	double[] inputLayer;
	double inputBias;
	double[] hiddenLayer;  // only one needed, could generalize to more.
	double hiddenBias;
	double[] outputLayer;
	
	int nNodesInputLayer = 0;
	int nNodesHiddenLayer = 0;
	int nNodesOutputLayer;
	
	int nTrainValRows;  // number of instances in the full training set, before split into train and val sets.
	int nTrainRows;  // number of instances in the training set, after validation set is removed.
	int nFeatures;
	
	final double LEARNING_RATE = .01;
	final double VAL_SPLIT = .2;  // 20% of the training data will be used for a validation set.
	final int N_HIDDEN_NODES = 2;  // number of nodes in the hidden layer, decided thru experimentation

	// do we assume a classification task? 
	// make it work for regression...?

	public NeuralNet(Random rand) {
		this.random = rand;
	}

	@Override
	public void train(DataMatrix featuresOnlyDataMatrix, DataMatrix labelsOnlyDataMatrix) throws Exception {
		// convert labels 2, 3, etc to 001 or 0010 (one hot vector)
		// use one hot version to calculate the error (100 010 version, per digit.)
		// reference the "network equations" in backprop slides
		
		// separate out train and val sets:
		featuresOnlyDataMatrix.shuffleRowOrderWithBuddyMatrix(random, labelsOnlyDataMatrix);
		nTrainValRows = featuresOnlyDataMatrix.getRowCount();
		nTrainRows = (int)((1- VAL_SPLIT) * nTrainValRows) ;
		nFeatures = featuresOnlyDataMatrix.getColCount();
		// DM params: DM otherMatrix, int rowStart, int colStart, int rowCount, int colCount
		DataMatrix xTrain = new DataMatrix(featuresOnlyDataMatrix, 0, 0, nTrainRows, nFeatures);
		DataMatrix yTrain = new DataMatrix(featuresOnlyDataMatrix, 0, nFeatures, nTrainRows, 1);
		DataMatrix xVal = new DataMatrix(featuresOnlyDataMatrix, nTrainRows, 0, nTrainValRows - nTrainRows, nFeatures);
		DataMatrix yVal = new DataMatrix(featuresOnlyDataMatrix, nTrainRows, nFeatures, nTrainValRows - nTrainRows, 1);
		
		initTopology(featuresOnlyDataMatrix, labelsOnlyDataMatrix);
		
		// initialize weights and biases:
		inputLayer = initWeights(nNodesInputLayer);
		inputBias = initWeights(1)[0];
		hiddenLayer = initWeights(nNodesHiddenLayer);
		hiddenBias = initWeights(1)[0];
		inputLayer = initWeights(nNodesInputLayer);
	}

	private void initTopology(DataMatrix x, DataMatrix y) {
		int nPossibleVals;
		for (int col = 0; col < x.getColCount(); col++) {
			nPossibleVals = x.getValueCountForAttributeAtColumn(col);
			nNodesInputLayer += (nPossibleVals == 0) ? 1 : nPossibleVals;      
		}
		nPossibleVals = y.getValueCountForAttributeAtColumn(0);
		nNodesOutputLayer += (nPossibleVals == 0) ? 1 : nPossibleVals;      
		// i.e iris - 1 output feature, 1 hot vector w 3 output nodes (100 for example for 1st class.)
		// but will put 0 1 or 2 in the arrayInWhichToPutLabels...
		// need the 1hot format to 
	}
	
	private double[] initWeights(int nInputs) {
	    double[] weights = new double[nInputs];
	    final double bound = 0.25;  // is this different for backprop?
	    for (int i = 0; i < weights.length; i++)
	    	weights[i] = (random.nextDouble() * 2 * bound) - bound;
	    return weights;
	}

	// arrayToPutLabels is 1 column. put prediction for one instance at that row. 
	// for classification, put in an integer that corresponds to the class
	// (0, 1, 2 for example)
	// BUT in model we should have as many output nodes we'd need based on the data
	// then we'd interpret / translate that (which has the highest net, for example
	// the translated version 001 would go into arrayToPutLabels
	
	// how to determine num possible outputs / output nodes for classification? ( 3 for iris)
	// for classification, num unique labels in the training labels
	// 
	@Override
	public void predictInstanceLabelsFromFeatures(double[] featureVector, double[] arrayInWhichToPutLabels)
			throws Exception {
		// TODO Auto-generated method stub

	}
}
