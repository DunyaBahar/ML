import java.util.Random;

// topology agnostic
// dont have training set, don't know # features or input nodes
// dont know what target label or output node is
// can't set up topology in constructor 
public class NeuralNet extends SupervisedLearner {
	Random random;
	
	// activations?
	double[] inputLayer;
	double[] hiddenLayer;  // only one needed, could generalize to more.
	double[] outputLayer;
	
	// weights
	double inputBias;
	double hiddenBias;
	
	int nNodesInputLayer = 0;
	int nNodesHiddenLayer = 0;
	int nNodesOutputLayer;
	
	int nTrainValRows;  // number of instances in the full training set, before split into train and val sets.
	int nTrainRows;  // number of instances in the training set, after validation set is removed.
	int nFeatures;
	
	final double LEARNING_RATE = .01;
	final double VAL_SPLIT = .2;  // 20% of the training data will be used for a validation set.
	final int N_HIDDEN_NODES = 2;  // number of nodes in the hidden layer, decided thru experimentation

	// do we assume a classification task? 
	// make it work for regression...?
	
	double[] prediction;  // This only ever contains 1 value. It's only an array because the toolkit expects it to be.

	public NeuralNet(Random rand) {
		this.random = rand;
	}

	@Override
	public void train(DataMatrix featuresOnlyDataMatrix, DataMatrix labelsOnlyDataMatrix) throws Exception {
		// convert labels 2, 3, etc to 001 or 0010 (one hot vector)
		// use one hot version to calculate the error (100 010 version, per digit.)
		// reference the "network equations" in backprop slides
		
		// separate out train and val sets:
		featuresOnlyDataMatrix.shuffleRowOrderWithBuddyMatrix(random, labelsOnlyDataMatrix);
		nTrainValRows = featuresOnlyDataMatrix.getRowCount();
		nTrainRows = (int)((1- VAL_SPLIT) * nTrainValRows) ;
		nFeatures = featuresOnlyDataMatrix.getColCount();
		// DM params: DM otherMatrix, int rowStart, int colStart, int rowCount, int colCount
		DataMatrix xTrain = new DataMatrix(featuresOnlyDataMatrix, 0, 0, nTrainRows, nFeatures);
		DataMatrix yTrain = new DataMatrix(featuresOnlyDataMatrix, 0, nFeatures, nTrainRows, 1);
		DataMatrix xVal = new DataMatrix(featuresOnlyDataMatrix, nTrainRows, 0, nTrainValRows - nTrainRows, nFeatures);
		DataMatrix yVal = new DataMatrix(featuresOnlyDataMatrix, nTrainRows, nFeatures, nTrainValRows - nTrainRows, 1);
		
		initTopology(featuresOnlyDataMatrix, labelsOnlyDataMatrix);
		
		// initialize weights and biases:
		inputLayer = initWeights(nNodesInputLayer);
		inputBias = initWeights(1)[0];
		hiddenLayer = initWeights(nNodesHiddenLayer);
		hiddenBias = initWeights(1)[0];
		inputLayer = initWeights(nNodesInputLayer);
		
		// fwd pass - call recall fn
		// predict()
		// get error at output nodes
		
		// backward pass:
		// calculate error at hidden nodes, based on the
		//   error of the ouput nodes which is propagated back to the hidden nodes.
		// continue propagating error back until the input layer is reached
		// update all weights based on the standard delta rule with the appropriate error fn d:
		//    change in weight_ij = C * d_j * Z_i
		//    where C is LR, and Z_i is the value of the ith node (works for any layer)
		
		
		// derivative of sigmoid:
		// f'(net_j) = Z_j * (1 - Z_j)
		
		boolean learning = true;  // flag for stopping criterion
		while (learning) {
			double[] instance;
			double trueLabel;
			double error;
			double activation;
			double gradient;
			// thru each instance:
			for (int i = 0; i < featuresOnlyDataMatrix.getRowCount(); i++) {
				instance = featuresOnlyDataMatrix.getRow(i);
				predictInstanceLabelsFromFeatures(instance, prediction);
				
				trueLabel = labelsOnlyDataMatrix.getRow(i)[0];
				// error for output nodes:
				error = LEARNING_RATE * (trueLabel - prediction[0]) * deriv(net);
				activation = sigmoidActivation(net);
				gradient = 1;
			}  // end for i
		}  // end while

	}
	
	// converts toolkit labels to one-hot encodings, for use in training
	private double[] oneHot(int label) {
		double[] oneHot = new double[nNodesOutputLayer];
		int i = 0;
		while (i < label) {
			oneHot[i] = 0;
			i++;
		}
		oneHot[label] = 1;
		i++;
		while (i < oneHot.length)
			oneHot[i] = 0;
	}
	
	// will return a number between 0 and 1
	private double sigmoidActivation(double net) {
		// 1 / (1 + e ^ (-net_j))
		double bottom = 1 + Math.pow(Math.E, -net);
		return 1/bottom;
	}

	// determines the number of nodes in the input and output layers
	private void initTopology(DataMatrix x, DataMatrix y) {
		int nPossibleVals;
		for (int col = 0; col < x.getColCount(); col++) {
			nPossibleVals = x.getValueCountForAttributeAtColumn(col);
			nNodesInputLayer += (nPossibleVals == 0) ? 1 : nPossibleVals;      
		}
		nPossibleVals = y.getValueCountForAttributeAtColumn(0);
		nNodesOutputLayer += (nPossibleVals == 0) ? 1 : nPossibleVals;    
		prediction = new double[nNodesOutputLayer];
		// i.e iris - 1 output feature, 1 hot vector w 3 output nodes (100 for example for 1st class.)
		// but will put 0 1 or 2 in the arrayInWhichToPutLabels...
		// need the 1hot format to 
	}
	
	// returns small random weights with mean 0
	private double[] initWeights(int nInputs) {
	    double[] weights = new double[nInputs];
	    final double bound = 0.25;  // could do 1 / sqrt(nNodesInputLayer) from pg. 80
	    for (int i = 0; i < weights.length; i++)
	    	weights[i] = (random.nextDouble() * 2 * bound) - bound;
	    return weights;
	}

	// arrayToPutLabels is 1 column. put prediction for one instance at that row. 
	// for classification, put in an integer that corresponds to the class
	// (0, 1, 2 for example)
	// BUT in model we should have as many output nodes we'd need based on the data
	// then we'd interpret / translate that (which has the highest net, for example
	// the translated version 001 would go into arrayToPutLabels
	
	// how to determine num possible outputs / output nodes for classification? ( 3 for iris)
	// for classification, num unique labels in the training labels
	// 
	@Override
	public void predictInstanceLabelsFromFeatures(double[] featureVector, double[] arrayInWhichToPutLabels) throws Exception {
		// fwd pass thru the network
		// from inputs to hidden layer:
		for (int i = 0; i < nNodesInputLayer; i++) { // iterate through the weights from each input to each hidden node:
			double net = 0;
			for (int j = 0; j < nNodesHiddenLayer; j++) {
				net += 
			}  // end j
		}  // end i
			net += weights[i] * featureVector[i];
		net += bias;
		arrayInWhichToPutLabels[0] = net > 0 ? 1 : 0;  // threshold activation function:
	}
}

