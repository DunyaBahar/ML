import java.util.Random;

// topology agnostic
// dont have training set, don't know # features or input nodes
// dont know what target label or output node is
// can't set up topology in constructor 
public class NeuralNet extends SupervisedLearner {
	Random random;
	
	double[] inputLayer;
	double[] hiddenLayer;
	double[] outputLayer;
	
	// weights
	double hiddenBias;
	double outputBias;
	double[][] hiddenWeights;
	double[][] outputWeights;
	
	int nNodesInputLayer = 0;
	int nNodesHiddenLayer = 0;
	int nNodesOutputLayer;
	
	int nTrainValRows;  // number of instances in the full training set, before split into train and val sets.
	int nTrainRows;  // number of instances in the training set, after validation set is removed.
	int nFeatures;
	
	final double LEARNING_RATE = .01;
	final double VAL_SPLIT = .2;  // 20% of the training data will be used for a validation set.
	final int N_HIDDEN_NODES = 2;  // number of nodes in the hidden layer, decided thru experimentation

	// do we assume a classification task? 
	// make it work for regression...?
	
	double[] prediction;  // This only ever contains 1 value. It's only an array because the toolkit expects it to be.
	double net = 0;

	
	public NeuralNet(Random rand) {
		this.random = rand;
	}

	@Override
	public void train(DataMatrix featuresOnlyDataMatrix, DataMatrix labelsOnlyDataMatrix) throws Exception {
		// convert labels 2, 3, etc to 001 or 0010 (one hot vector)
		// use one hot version to calculate the error (100 010 version, per digit.)
		// reference the "network equations" in backprop slides
		
		// separate out train and val sets:
		featuresOnlyDataMatrix.shuffleRowOrderWithBuddyMatrix(random, labelsOnlyDataMatrix);
		nTrainValRows = featuresOnlyDataMatrix.getRowCount();
		nTrainRows = (int)((1- VAL_SPLIT) * nTrainValRows) ;
		nFeatures = featuresOnlyDataMatrix.getColCount();
		// DM params: DM otherMatrix, int rowStart, int colStart, int rowCount, int colCount
		DataMatrix xTrain = new DataMatrix(featuresOnlyDataMatrix, 0, 0, nTrainRows, nFeatures);
		DataMatrix yTrain = new DataMatrix(featuresOnlyDataMatrix, 0, nFeatures, nTrainRows, 1);
		DataMatrix xVal = new DataMatrix(featuresOnlyDataMatrix, nTrainRows, 0, nTrainValRows - nTrainRows, nFeatures);
		DataMatrix yVal = new DataMatrix(featuresOnlyDataMatrix, nTrainRows, nFeatures, nTrainValRows - nTrainRows, 1);
		
		initTopology(featuresOnlyDataMatrix, labelsOnlyDataMatrix);
		initWeights();
		// backward pass:
		// calculate error at hidden nodes, based on the
		//   error of the ouput nodes which is propagated back to the hidden nodes.
		// continue propagating error back until the input layer is reached
		// update all weights based on the standard delta rule with the appropriate error fn d:
		//    change in weight_ij = C * d_j * Z_i
		//    where C is LR, and Z_i is the value of the ith node (works for any layer)
		
		boolean learning = true;  // flag for stopping criterion
		while (learning) {
			double[] instance;
			double trueLabel;
			double outputActivation;
			double hiddenActivation;
			double gradient;
			double[] oneHotPred;
			double[] oneHotLabel;
			double delta;
			double outputError;
			// thru each instance:
			for (int i = 0; i < featuresOnlyDataMatrix.getRowCount(); i++) {
				instance = featuresOnlyDataMatrix.getRow(i);
				predictInstanceLabelsFromFeatures(instance, prediction);
				oneHotPred = oneHot(prediction[i]);
				oneHotLabel = oneHot(labelsOnlyDataMatrix.getRow(i)[0]);
				// thru each output node:
				for (int j = 0; j < nNodesOutputLayer; j++) {
					outputError = (oneHotLabel[j] - oneHotPred[0]) * gradient(net);
					delta = LEARNING_RATE * sigmoidActivation(net) * outputError;
					outputWeights[i][j] += delta;
					// thru each hidden node:
					for (int h = 0; h < N_HIDDEN_NODES; h++) {
						
					}  // end for h
				}  // end for j
			}  // end for i
		}  // end while
	}
	
	// calculates the derivative of sigmoid of the net
	// f'(net_j) = Z_j * (1 - Z_j)
	public double gradient(double net_j) {
		double O_j = sigmoidActivation(net_j);
		return O_j * (1 - O_j);
	}
	
	// converts toolkit labels to one-hot encodings, for use in training
	private double[] oneHot(double label) {
		double[] oneHot = new double[nNodesOutputLayer];
		int i = 0;
		while (i < label) {
			oneHot[i] = 0;
			i++;
		}
		oneHot[(int)label] = 1;
		i++;
		while (i < oneHot.length) {
			oneHot[i] = 0;
			i++;
		}
		return oneHot;
	}
	
	// will return a number between 0 and 1
	private double sigmoidActivation(double net_j) {
		// 1 / (1 + e ^ (-net_j))
		double bottom = 1 + Math.pow(Math.E, -net_j);
		return 1/bottom;
	}

	// determines the number of nodes in the input and output layers
	private void initTopology(DataMatrix x, DataMatrix y) {
		int nPossibleVals;
		for (int col = 0; col < x.getColCount(); col++) {
			nPossibleVals = x.getValueCountForAttributeAtColumn(col);
			nNodesInputLayer += (nPossibleVals == 0) ? 1 : nPossibleVals;      
		}
		nPossibleVals = y.getValueCountForAttributeAtColumn(0);
		nNodesOutputLayer += (nPossibleVals == 0) ? 1 : nPossibleVals;    
		prediction = new double[nNodesOutputLayer];
		// i.e iris - 1 output feature, 1 hot vector w 3 output nodes (100 for example for 1st class.)
		// but will put 0 1 or 2 in the arrayInWhichToPutLabels...
		// need the 1hot format to 
	}
	
	// returns small random weights with mean 0
	private void initWeights() {
	    final double bound = 0.25;  // could do 1 / sqrt(nNodesInputLayer) from pg. 80
	    for (int i = 0; i < nNodesHiddenLayer; i++)
	    	for (int j = 0; j < nNodesOutputLayer; j++)
	    		outputWeights[i][j] = (random.nextDouble() * 2 * bound) - bound;
	    for (int i = 0; i < nNodesInputLayer; i++)
	    	for (int j = 0; j < nNodesHiddenLayer; j++)
	    		hiddenWeights[i][j] = (random.nextDouble() * 2 * bound) - bound;
	    hiddenBias = (random.nextDouble() * 2 * bound) - bound;
	    outputBias = (random.nextDouble() * 2 * bound) - bound;
	}

	// arrayToPutLabels is 1 column. put prediction for one instance at that row. 
	// for classification, put in an integer that corresponds to the class
	// (0, 1, 2 for example)
	// BUT in model we should have as many output nodes we'd need based on the data
	// then we'd interpret / translate that (which has the highest net, for example
	// the translated version 001 would go into arrayToPutLabels
	
	// how to determine num possible outputs / output nodes for classification? ( 3 for iris)
	// for classification, num unique labels in the training labels
	// 
	@Override
	public void predictInstanceLabelsFromFeatures(double[] featureVector, double[] arrayInWhichToPutLabels) throws Exception {
		// fwd pass thru the network
		// from inputs to hidden layer:
		for (int i = 0; i < nNodesInputLayer; i++) { // iterate through the weights from each input to each hidden node:
			double net = 0;
			for (int j = 0; j < nNodesHiddenLayer; j++) {
				net += 6;
			}  // end j
		}  // end i
			net += weights[i] * featureVector[i];
		net += bias;
		this.net = net;
		arrayInWhichToPutLabels[0] = net > 0 ? 1 : 0;  // threshold activation function:
	}
}




